# what we are going to cover

1. Architecture of neural network
2. input shape and output shapes of `classfication` model (feature and label)
3. creating custom data to view and fit
4.  steps in modelling
	* creating a model, compiling a model, fitting a model, evaluating a model
5. Different `classfication` evaluation methods
6. Saving and loading the model

![[Pasted image 20230719130938.png]]

## classification inputs and outputs

![[Pasted image 20230719131032.png]]

## Input and output share

![[Pasted image 20230719131120.png]]

### Activation functions:

1. Example of the Activation layer

```py
# create a toy a tensor (similar to the data we pass into our model)

toy_tensor = tf.cast(tf.range(-10, 10), tf.float32)

# Visualize our toy tensor

plt.plot(toy_tensor)
```

2. Sigmoid function
```py
# lets build our own sigmoid function

def sigmoid(z):
    return 1/(1+np.exp(-z))

# use our sigmoid function on our data

sigmoid_preds = sigmoid(toy_tensor)
sigmoid_preds
```
![[Pasted image 20230811210321.png]]

3. Relu function
```py
# Let's recreate the relu function

def relu(x):
	return tf.maximum(0, x)

# pass a toy tensor
relu(toy_tensor)
```

4.  linear activation function

```py
# plot linear activation function

plt.plot(tf.keras.activations.linear(toy_tensor))


plt.xlabel('Values')
plt.ylabel('Linear ')
plt.title('Linear activation function')

plt.legend()

plt.show()
```

![[Pasted image 20230811211404.png]]

 
 [activation functions notes](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)

**ü§î Question:** : what's wrong the predictions we've made ? Are we really evaluating our model correctly ? Hint: what data did the model learn on and what data did we predict on ?

  

**üîê Note**: The combination of **linear (straight lines) and non-linear functions** is one of the key fundamentals of neural networks.

