

correlation :
	Correlation explains how one or more variables are related to each other.

**for more** ![[5.Correlation]]

 # What is Logistic Regression?
 
==It’s a classification algorithm, that is used where the response variable is== ==_categorical_====. The idea of Logistic Regression is to find a ==**relationship between features and probability of particular outcome**==

> **Pros**

- Simple and efficient.
- Low variance.
- It provides **probability** score for observations.

> **Cons:**

- Doesn’t handle **large** number of categorical features/variables well.
- It requires transformation of non-linear features.
reference : [Logisitic Regression](https://medium.com/data-science-group-iitr/logistic-regression-simplified-9b4efe801389)



# K-Nearest Neighbor

![[Pasted image 20230531165303.png]]
K-nearest neighbors (KNN) is a type of supervised learning algorithm used for both regression and classification. KNN tries to predict the correct class for the test data by calculating the distance between the test data and all the training points. Then select the K number of points which is closet to the test data. The KNN algorithm calculates the probability of the test data belonging to the classes of ‘K’ training data and class holds the highest probability will be selected. In the case of regression, the value is the mean of the ‘K’ selected training points.

Let see the below example to make it a better understanding

![](https://miro.medium.com/v2/resize:fit:875/0*34SajbTO2C5Lvigs.png)



reference : [k-nearest](https://medium.com/swlh/k-nearest-neighbor-ca2593d7a3c4)

 ## Logistic Regression Hyperparameters

The main hyperparameters we may tune in logistic regression are: solver, penalty, and regularization strength ([sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)).

**Solver** is the algorithm to use in the optimization problem. The choices are _{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}_, default=’lbfgs’.

1. `lbfgs` relatively performs well compared to other methods and it saves a lot of memory, however, sometimes it may have issues with convergence.
2. `sag` faster than other solvers for large datasets, when both the number of samples and the number of features are large.
3. `saga` the solver of choice for sparse multinomial logistic regression and it’s also suitable for very large datasets.
4. `newton-cg` computationally expensive because of the Hessian Matrix.
5. `liblinear`recommended when you have a high dimension dataset - solving large-scale classification problems.


reference : [Logistic Hyperparameter](https://medium.com/codex/do-i-need-to-tune-logistic-regression-hyperparameters-1cb2b81fca69)

# Classification Reposrt


`Precision` — _What percent of your predictions were correct?_

Precision is the ability of a classifier not to label an instance positive that is actually negative. For each class, it is defined as the ratio of true positives to the sum of a true positive and false positive.

Precision:- Accuracy of positive predictions.

Precision = TP/(TP + FP)

`Recall` — _What percent of the positive cases did you catch?_

Recall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives.

Recall:- **Fraction of positives that were correctly identified**.

Recall = TP/(TP+FN)

`F1 score` —**_What percent of positive predictions were correct?_**

The F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. F1 scores are lower than accuracy measures as they embed precision and recall into their computation. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.

F1 Score = 2*(Recall * Precision) / (Recall + Precision)
reference : [classification Report](https://medium.com/@kohlishivam5522/understanding-a-classification-report-for-your-machine-learning-model-88815e2ce397)

